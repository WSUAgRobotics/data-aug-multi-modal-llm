## Data-Aug-Multi-Modal-LLM  
# A Comprehensive Survey on Text, Audio, and Image Data Augmentation Using Multi-Modal LLMs for Deep Learning Applications

This repo contains all the relevant paper and information used in our study. This will be updated perodically as we revise our manuscript throughout the publication process.
![Google Search Trend](pics/google_trend.png)


The papers used in this study are organised and the links can be found below:

# Text Data Augmentation

## Peer Reviewed Paper


1. Ahmed, T., Pai, K. S., Devanbu, P., & Barr, E. Automatic semantic augmentation of language model prompts (for code summarization). Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, 2024. [paper](https://dl.acm.org/doi/10.1145/3597503.3639183) [code](#) - NOCODE

1. Cai, X., Xiao, M., Ning, Z., & Zhou, Y. Resolving the imbalance issue in hierarchical disciplinary topic inference via LLM-based data augmentation. 2023 IEEE International Conference on Data Mining Workshops (ICDMW), 2023. [paper](https://ieeexplore.ieee.org/document/10415709) [code](#) - NOCODE

1. Cloutier, N. A., & Japkowicz, N. Fine-tuned generative LLM oversampling can improve performance over traditional techniques on multiclass imbalanced text classification. 2023 IEEE International Conference on Big Data (BigData), 2023. [paper](#) [code](#)

1. Santos, V. G., Santos, G. L., Lynn, T., & Benatallah, B. Identifying citizen-related issues from social media using LLM-based data augmentation. International Conference on Advanced Information Systems Engineering, 2024. [paper](#) [code](#)

1. Hu, L., He, H., Wang, D., Zhao, Z., Shao, Y., & Nie, L. LLM vs small model? Large language model-based text augmentation enhanced personality detection model. Proceedings of the AAAI Conference on Artificial Intelligence, 2024. [paper](#) [code](#)

1. Hua, J., Cui, X., Li, X., Tang, K., & Zhu, P. Multimodal fake news detection through data augmentation-based contrastive learning. Applied Soft Computing, 2023. [paper](#) [code](#)

1. Jung, H., Yeen, H., Lee, J., Kim, M., Bang, N., & Koo, M.-W. Enhancing task-oriented dialog system with subjective knowledge: A large language model-based data augmentation framework. Proceedings of The Eleventh Dialog System Technology Challenge, 2023. [paper](#) [code](#)

1. Lai, J., Yang, X., Luo, W., Zhou, L., Li, L., Wang, Y., & Shi, X. RumorLLM: A rumor large language model-based fake-news-detection data-augmentation approach. Applied Sciences, 2024. [paper](#) [code](#)

1. Meng, Z., Liu, T., Zhang, H., Feng, K., & Zhao, P. CEAN: Contrastive event aggregation network with LLM-based augmentation for event extraction. Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, 2024. [paper](#) [code](#)

1. Silva, K., Frommholz, I., Can, B., Blain, F., Sarwar, R., & Ugolini, L. Forged-GAN-BERT: Authorship attribution for LLM-generated forged novels. Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, 2024. [paper](#) [code](#)

1. Wan, M., Safavi, T., Jauhar, S. K., Kim, Y., Counts, S., Neville, J., Suri, S., Shah, C., White, R. W., Yang, L., & others. TnT-LLM: Text mining at scale with large language models. Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024. [paper](#) [code](#)

1. Wu, S.-L., Chang, X., Wichern, G., Jung, J.-W., Germain, F., Le Roux, J., & Watanabe, S. Improving audio captioning models with fine-grained audio features, text embedding supervision, and LLM mix-up augmentation. ICASSP 2024 IEEE International Conference on Acoustics, Speech and Signal Processing, 2024. [paper](#) [code](#)

1. Zhang, J., Gao, H., Zhang, P., Feng, B., Deng, W., & Hou, Y. LA-UCL: LLM-augmented unsupervised contrastive learning framework for few-shot text classification. Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024. [paper](#) [code](#)

1. Zhang, M., Jiang, G., Liu, S., Chen, J., & Zhang, M. LLM–assisted data augmentation for Chinese dialogue–level dependency parsing. Computational Linguistics, 2024. [paper](#) [code](#)

1. Zhao, H., Chen, H., Ruggles, T. A., Feng, Y., Singh, D., & Yoon, H.-J. Improving text classification with large language model-based data augmentation. Electronics, 2024. [paper](#) [code](#)


## Preprints

1. Kang, A., Chen, J. Y., Lee-Youngzie, Z., & Fu, S. Synthetic data generation with LLM for improved depression prediction. arXiv preprint arXiv:2411.17672, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Song, S., Subramanyam, A., Madejski, I., & Grossman, R. L. Lab-RAG: Label boosted retrieval augmented generation for radiology report generation. arXiv preprint arXiv:2411.16523, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Fischer, L., Gao, Y., Lintner, A., & Ebling, S. SwissADT: An audio description translation system for Swiss languages. arXiv preprint arXiv:2411.14967, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Glazkova, A., & Zakharova, O. Evaluating LLM prompts for data augmentation in multi-label classification of ecological texts. arXiv preprint arXiv:2411.14896, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Wen, Z., Guo, D., & Zhang, H. AIDBench: A benchmark for evaluating the authorship identification capability of large language models. arXiv preprint arXiv:2411.13226, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Alyafeai, Z., et al. Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic. arXiv preprint arXiv:2412.04277, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Liu, J., & Nguyen, A. Rephrasing electronic health records for pretraining clinical language models. arXiv preprint arXiv:2411.18940, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Abane, A., Bekri, A., & Battou, A. FastRAG: Retrieval augmented generation for semi-structured data. arXiv preprint arXiv:2411.13773, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Yang, M., Shi, B., Le, M., et al. AudioBox TTA-RAG: Improving zero-shot and few-shot text-to-audio with retrieval-augmented generation. arXiv preprint arXiv:2411.05141, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Fuad, K. A. A., & Chen, L. LLM-Ref: Enhancing reference handling in technical writing with large language models. arXiv preprint arXiv:2411.00294, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Wang, Z., Xu, G., & Ren, M. LLM-generated natural language meets scaling laws: New explorations and data augmentation methods. arXiv preprint arXiv:2407.00322, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Dai, H., Liu, Z., & Wu, Z. AugGPT: Leveraging ChatGPT for text data augmentation. arXiv preprint arXiv:2302.13007, 2023. [paper](#) <span style="color: red;">NOCODE</span>

1. Cegin, J., Simko, J., & Brusilovsky, P. LLMs vs established text augmentation techniques for classification: When do the benefits outweigh the costs? arXiv preprint arXiv:2408.16502, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Lee, N., Wattanawong, T., Kim, S., et al. LLM2LLM: Boosting LLMs with novel iterative data enhancement. arXiv preprint arXiv:2403.15042, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Song, Y., Zhang, J., Tian, Z., et al. LLM-based privacy data augmentation guided by knowledge distillation with a distribution tutor for medical text classification. arXiv preprint arXiv:2402.16515, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Yang, H., Zhao, X., Huang, S., et al. LATEX-GCL: Large language models (LLMs)-based data augmentation for text-attributed graph contrastive learning. arXiv preprint arXiv:2409.01145, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Liu, Y., Zhu, Y., Gu, Z., et al. Improving topic relevance model by mix-structured summarization and LLM-based data augmentation. arXiv preprint arXiv:2404.02616, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Cegin, J., Pecher, B., Simko, J., et al. Use random selection for now: Investigation of few-shot selection strategies in LLM-based text augmentation for classification. arXiv preprint arXiv:2410.10756, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Jia, K., Wu, Y., & Li, R. Curriculum-style data augmentation for LLM-based metaphor detection. arXiv preprint arXiv:2412.02956, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Jung, K., Seo, Y., Cho, S., et al. DALDA: Data augmentation leveraging diffusion model and LLM with adaptive guidance scaling. arXiv preprint arXiv:2409.16949, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Cegin, J., Pecher, B., Simko, J., et al. Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation. arXiv preprint arXiv:2401.06643, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Zeng, L. Leveraging large language models for code-mixed data augmentation in sentiment analysis. arXiv preprint arXiv:2411.00691, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Litake, O., Yagnik, N., & Labhsetwar, S. Inditext boost: Text augmentation for low resource Indian languages. arXiv preprint arXiv:2401.13085, 2024. [paper](#) <span style="color: red;">NOCODE</span>

1. Sahu, G., Vechtomova, O., Bahdanau, D., & Laradji, I. H. PromptMix: A class boundary augmentation method for large language model distillation. arXiv preprint arXiv:2310.14192, 2023. [paper](#) <span style="color: red;">NOCODE</span>

1. Chowdhury, A. G., & Chadha, A. Generative data augmentation using LLMs improves distributional robustness in question answering. arXiv preprint arXiv:2309.06358, 2023. [paper](#) <span style="color: red;">NOCODE</span>

1. Wang, L., Yu, L., Zhang, Y., & Xie, H. Large language model-based augmentation for imbalanced node classification on text-attributed graphs. arXiv preprint arXiv:2410.16882, 2024. [paper](#) <span style="color: red;">NOCODE</span>


# Image Data Augmentation

## Peer Reviewed Paper

## Prepreints


# AUDIO/VOICE DATA AUGMENTATION

## Peer Reviewed Paper

## Prepreints
